{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\t7 [Discriminative\tvs\tGenerative\tModels,\t25\tMarks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Load\t Task1E_train.csv and\t Task1E_test.csv as\t well\t as\t the\t Bayesian classifier\t(BC)\tand\tlogistic\tregression\t(LR)\tcodes\tfrom\tActivities\t2\tand\t3\tin\tModule\t3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "library(mvtnorm) # generates multivariate Gaussian sampels and calculate the densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data7.train <- read.csv(\"./Task1E_train.csv\")\n",
    "data7.test <- read.csv(\"./Task1E_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian classifier (BC)\n",
    "# refers to tutorial code\n",
    "BayesianClassifier <- function(train.data,train.label,test.data){\n",
    "    \n",
    "    # Class probabilities:\n",
    "    p0.hat <- sum(train.label==1)/nrow(train.data) # total number of samples in class 0 divided by the total nmber of training data\n",
    "    p1.hat <- sum(train.label==-1)/nrow(train.data) # or simply 1 - p1.hat\n",
    "\n",
    "    # Class means:\n",
    "    mu0.hat <- colMeans(train.data[train.label==1,])\n",
    "    mu1.hat <- colMeans(train.data[train.label==-1,])\n",
    "    \n",
    "    # class covariance matrices:\n",
    "    sigma0.hat <- var(train.data[train.label==1,])\n",
    "    sigma1.hat <- var(train.data[train.label==-1,])\n",
    "    \n",
    "    # shared covariance matrix:\n",
    "    sigma.hat <- p0.hat * sigma0.hat + p1.hat * sigma1.hat \n",
    "    \n",
    "    # calculate posteriors:\n",
    "    posterior0 <- p0.hat*dmvnorm(x=train.data, mean=mu0.hat, sigma=sigma.hat)\n",
    "    posterior1 <- p1.hat*dmvnorm(x=train.data, mean=mu1.hat, sigma=sigma.hat)\n",
    "    \n",
    "    # Predict on testing data\n",
    "    test.predict <- ifelse(p0.hat*dmvnorm(x=test.data, mean=mu0.hat, sigma=sigma.hat) > p1.hat*dmvnorm(x=test.data, mean=mu1.hat, sigma=sigma.hat), 1, -1)\n",
    "\n",
    "    return(test.predict)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (LR)\n",
    "# refers to tutorial code\n",
    "\n",
    "LogisticRegresson <- function(train.data,train.label,test.data){\n",
    " \n",
    "    # auxiliary function that predicts class labels\n",
    "    predict <- function(w, X, c0, c1){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(ifelse(sig>0.5, c1,c0))\n",
    "    }\n",
    "    \n",
    "    # auxiliary function that calculate a cost function\n",
    "    cost <- function (w, X, T, c0){\n",
    "    sig <- sigmoid(w, X)\n",
    "    return(sum(ifelse(T==c0, 1-sig, sig)))\n",
    "    }\n",
    "    \n",
    "    # Sigmoid function (=p(C1|X))\n",
    "    sigmoid <- function(w, x){\n",
    "    return(1.0/(1.0+exp(-w%*%t(cbind(1,x)))))    \n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Initializations\n",
    "    c0 <- 1; c1 <- -1\n",
    "    tau.max <- 1000 # maximum number of iterations\n",
    "    eta <- 0.01 # learning rate\n",
    "    epsilon <- 0.01 # a threshold on the cost (to terminate the process)\n",
    "    tau <- 1 # iteration counter\n",
    "    terminate <- FALSE\n",
    "\n",
    "    ## Just a few name/type conversion to make the rest of the code easy to follow\n",
    "    X <- as.matrix(train.data) # rename just for conviniance\n",
    "    T <- ifelse(train.label==c0,0,1) # rename just for conviniance\n",
    "\n",
    "    W <- matrix(,nrow=tau.max, ncol=(ncol(X)+1)) # to be used to store the estimated coefficients\n",
    "    W[1,] <- runif(ncol(W)) # initial weight (any better idea?)\n",
    "\n",
    "    # project data using the sigmoid function (just for convenient)\n",
    "    Y <- sigmoid(W[1,],X)\n",
    "\n",
    "    costs <- data.frame('tau'=1:tau.max)  # to be used to trace the cost in each iteration\n",
    "    costs[1, 'cost'] <- cost(W[1,],X,T, c0)\n",
    "    \n",
    "    \n",
    "    while(!terminate){\n",
    "    # check termination criteria:\n",
    "    terminate <- tau >= tau.max | cost(W[tau,],X,T, c0)<=epsilon\n",
    "    \n",
    "    # shuffle data:\n",
    "    train.index <- sample(1:nrow(train.data), nrow(train.data), replace = FALSE)\n",
    "    X <- X[train.index,]\n",
    "    T <- T[train.index]\n",
    "    \n",
    "    # for each datapoint:\n",
    "    for (i in 1:nrow(train.data)){\n",
    "        # check termination criteria:\n",
    "        if (tau >= tau.max | cost(W[tau,],X,T, c0) <=epsilon) {terminate<-TRUE;break}\n",
    "        \n",
    "        Y <- sigmoid(W[tau,],X)\n",
    "            \n",
    "        # Update the weights\n",
    "        W[(tau+1),] <- W[tau,] - eta * (Y[i]-T[i]) * cbind(1, t(X[i,]))\n",
    "        \n",
    "        # record the cost:\n",
    "        costs[(tau+1), 'cost'] <- cost(W[tau,],X,T, c0)\n",
    "        \n",
    "        # update the counter:\n",
    "        tau <- tau + 1\n",
    "        \n",
    "        # decrease learning rate:\n",
    "        eta = eta * 0.999\n",
    "        }\n",
    "    }\n",
    "    # Done!\n",
    "    costs <- costs[1:tau, ] # remove the NaN tail of the vector (in case of early stopping)\n",
    "\n",
    "    # the  final result is:\n",
    "    w <- W[tau,]\n",
    "    \n",
    "    return(predict(w,test.data,c0,c1))\n",
    "    \n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Using\t the\t first\t 5 data\t points\t from\t the\t training\t set,\t train\t a\t BC\t and\t a\t LR\tmodel,\tand\tcompute\ttheir\ttest\terrors.\tIn\ta\t“for\tloop”,\tincrease\tthe\tsize\tof\ttraining\t set\t (5 data\t points\t at\t a\t time),\t retrain\t the\t models\t and\t calculate\ttheir\ttest\terrors\tuntil\tall\ttraining\tdata\tpoints\tare\tused.\tIn\tone\tfigure,\tplot\tthe\ttest\terrors\tfor\teach\tmodel\t(with\tdifferent\tcolors)\tversus\tthe\tsize\tof\tthe\ttraining\tset;\tinclude\tthe\tplot\tin\tyour\tJupyter\tNotebook\tfile\tfor\tQuestion\t7.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error matirx\n",
    "test.error <- matrix(,nrow=nrow(data7.train)/5+1,ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create testing set\n",
    "test.data <- data7.test[,-3]\n",
    "test.label <- data7.test[,3]\n",
    "\n",
    "# initialze index and terminate variable\n",
    "i <- 5\n",
    "terminate <- FALSE\n",
    "iteration <- 1\n",
    "\n",
    "while (!terminate){\n",
    "    \n",
    "    if (i > nrow(data7.train)){\n",
    "        i <- nrow(data7.train)\n",
    "        terminate <- TRUE\n",
    "        }\n",
    "    \n",
    "    # Create training set and testing set\n",
    "    train.data <- data7.train[1:i,-3]\n",
    "    train.label <- data7.train[1:i,3]\n",
    "\n",
    "    # Training process\n",
    "    test.bayes.predict <- BayesianClassifier(train.data,train.label,test.data)\n",
    "    test.lr.predict <- LogisticRegresson(train.data,train.label,test.data)\n",
    "    \n",
    "    # Record test error\n",
    "    test.error[iteration,1] <- i\n",
    "    test.error[iteration,2] <- sum(test.label!=test.bayes.predict)/nrow(test.data)*100\n",
    "    test.error[iteration,3] <- sum(test.label!=test.lr.predict)/nrow(test.data)*100\n",
    "    \n",
    "    \n",
    "    # increase i by 5\n",
    "    i <- i + 5\n",
    "    \n",
    "    # increase iteration\n",
    "    iteration <- iteration + 1\n",
    "    \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>DataPoint</th><th scope=col>type</th><th scope=col>error</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5   </td><td>BC  </td><td>68.4</td></tr>\n",
       "\t<tr><th scope=row>102</th><td>5   </td><td>LR  </td><td> 9.8</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       "  & DataPoint & type & error\\\\\n",
       "\\hline\n",
       "\t1 & 5    & BC   & 68.4\\\\\n",
       "\t102 & 5    & LR   &  9.8\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | DataPoint | type | error | \n",
       "|---|---|\n",
       "| 1 | 5    | BC   | 68.4 | \n",
       "| 102 | 5    | LR   |  9.8 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    DataPoint type error\n",
       "1   5         BC   68.4 \n",
       "102 5         LR    9.8 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test.error <- as.data.frame(test.error)\n",
    "names(test.error) <- c('dataPoint','BC','LR')\n",
    "\n",
    "# Reshape the test error\n",
    "test.error.m <- melt(test.error,id='dataPoint')\n",
    "names(test.error.m) <- c('DataPoint','type','error')\n",
    "\n",
    "test.error.m[test.error.m$DataPoint==5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAOVBMVEUAAAAAv8RNTU1oaGh8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\nxp8AAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "ggplot(data=test.error.m,aes(x=DataPoint,y=error,color=type)) + geom_line() +\n",
    "     scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +\n",
    "      ggtitle(\"Test errors versus the size of the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Explain your observations in your Jupyter Notebook file.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. What does happen for each classifier when the number of training data points is increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Bayes classifier, if the number of training data is relatively small, the testing error will be large. Therefore, when increasing the number of training data, the testing error decreases to a lower level. Then keep increasing the number of training data, the testing error will fluctuate in a small range. Finally it will keep stable no matter how do we inlarge the dataset. \n",
    "\n",
    "For Logistic regression, similar with Bayes classifier, if the number of training data is small, the testing error will be large. When enlarging the dataset the testing error decreases significantly; then it will fluctate between a small range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Which classifier is best suited when the training set is small, and which is best suited when the training set is big?\n",
    "\n",
    "From the plot above, we can see:\n",
    "* When training set is small, the best classifier is Logistic regression.\n",
    "* When training set is large, Logistic regression performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Justify your observations in previous questions (III.a & III.b) by providing some speculations and possible reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question II.a\n",
    "\n",
    "1. If the training dataset is very small, for example it is only 4, obviously both methods are inaccurate. For Bayesian classifier, estimated parameters of Gussian distributions are not accurrate in terms of few empirical data and for logistic regression, there are less data points for the gradient descent.\n",
    "\n",
    "2. For a specific training dataset, the Bayesian classifier only has one model, thus for the same testing dataset the testing error always converges to the same value. However, logisitc regression absorbes the advantage of gradient descent to find the optimal parameters, therefore it may fluctuate around the optimal value for long time. But by the constrast, the asymptotic classification accuracy for Logistic Regression is often better than the asymptotic accuracy of Bayesian classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question III.b\n",
    "\n",
    "1. Bayesian Classifier uses Bayes Theorem to calculate the postoir probability $p(Y|X)$, while logistic regression directly calculate $p(Y|X)$. Note that the assumption of Bayesian classifier is that each $p(X|Y)$ is independent. If the training set is large, there may be more covariance between features therefore the so $p(X|Y)$ cannot always be independent. By contrast logistic regression will not be impacted by this situation because it uses gradient descent to estimate parameters.\n",
    "\n",
    "2. If the training dataset is samll, it is bad for logistic regression to estimate parameters while for Bayesian classifier, it performs better.\n",
    "\n",
    "3. There are D(D+5)/2 + 1 pararmeters for Bayesian classifier while D+1 parameters for logistic regression (D is the dimension of variables). In a large dataset the Batesian classifier is probabily overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
