{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 [Ridge Regression, 25 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Using R (with no use of special libraries), implement SGD and BGD algorithms that you derived in Step I. The implementation is straightforward as you are allowed to use the code examples from Activity 1 in Module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to calculate labels based on the estimated coefficients\n",
    "predict_func <- function(Phi, w){\n",
    "  return(Phi%*%w)\n",
    "} \n",
    "\n",
    "# auxiliary function to calculate the objective function for the training\n",
    "train_obj_func <- function (Phi, w, label, lambda){\n",
    "  # the L2 regulariser is already included in the objective function for training \n",
    "  return(mean((predict_func(Phi, w) - label)^2) + .5 * lambda * w %*% w)\n",
    "}\n",
    "\n",
    "# auxiliary function to compute the error of the model\n",
    "get_errors <- function(train_data, test_data, W) {\n",
    "  n_weights = dim(W)[1]\n",
    "  errors = matrix(,nrow=n_weights, ncol=2)\n",
    "  for (tau in 1:n_weights) {\n",
    "    errors[tau,1] = train_obj_func(train_data$x, W[tau,],train_data$y, 0)\n",
    "    errors[tau,2] = train_obj_func(test_data$x, W[tau,],test_data$y, 0)\n",
    "  }\n",
    "  return(errors)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent main function\n",
    "# refer ot assignment code of question 5\n",
    "sgd_train <- function(train_x, train_y, lambda, eta, epsilon, max_epoch) {\n",
    "  \n",
    "  train_len = dim(train_x)[1]\n",
    "  tau_max = max_epoch * train_len\n",
    "  \n",
    "  W <- matrix(,nrow=tau_max, ncol=ncol(train_x)) \n",
    "  W[1,] <- runif(ncol(train_x))\n",
    "  \n",
    "  tau = 1 # counter \n",
    "  obj_func_val <-matrix(,nrow=tau_max, ncol=1) \n",
    "  obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "  \n",
    "  while (tau <= tau_max){\n",
    "    \n",
    "    # check termination criteria\n",
    "    if (obj_func_val[tau,1]<=epsilon) {break}\n",
    "    \n",
    "    # shuffle data:\n",
    "    train_index <- sample(1:train_len, train_len, replace = FALSE)\n",
    "    \n",
    "    # loop over each datapoint\n",
    "    for (i in train_index) {\n",
    "      # increment the counter\n",
    "      tau <- tau + 1\n",
    "      if (tau > tau_max) {break}\n",
    "      \n",
    "      # make the weight update\n",
    "      y_pred <- predict_func(train_x[i,], W[tau-1,])\n",
    "      W[tau,] <- sgd_update_weight(W[tau-1,], train_x[i,], train_y[i], y_pred, lambda, eta)\n",
    "      \n",
    "      # keep track of the objective funtion\n",
    "      obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "    }\n",
    "  }\n",
    "  # resulting values for the training objective function as well as the weights\n",
    "  return(list('vals'=obj_func_val,'W'=W))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent updating the weight vector\n",
    "sgd_update_weight <- function(W_prev, x, y_true, y_pred, lambda, eta) {\n",
    "  # MODIFY THIS FUNCTION FOr L2 REG\n",
    "  grad = - (y_true-y_pred) * x \n",
    "  return(W_prev - eta * (grad+lambda*W_prev))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent main function\n",
    "# refer to assignment code of question 5\n",
    "bgd_train <- function(train_x, train_y, lambda, eta, epsilon, max_epoch) {\n",
    "  \n",
    "  train_len = dim(train_x)[1]\n",
    "  \n",
    "  W <- matrix(,nrow=(max_epoch+1), ncol=ncol(train_x))\n",
    "  W[1,] <- runif(ncol(train_x))\n",
    "  \n",
    "  tau = 1 # counter \n",
    "  obj_func_val <-matrix(,nrow=(max_epoch+1), ncol=1)\n",
    "  obj_func_val[tau,1] = train_obj_func(train_x, W[tau,],train_y, lambda)\n",
    "  \n",
    "  trainin_size = dim(train_x)[1]\n",
    "  for (tau in 1:max_epoch){\n",
    "    \n",
    "    # check termination criteria\n",
    "    if (obj_func_val[tau,1]<=epsilon) {break}\n",
    "    \n",
    "    # make prediction over the training set\n",
    "    y_pred = train_x %*% W[tau,]\n",
    "    \n",
    "    # update the weight using linear search\n",
    "    W[tau+1,] = bgd_update_weight(W[tau,], train_x, train_y, y_pred, lambda, eta)\n",
    "    \n",
    "    # keep track of the objective funtion\n",
    "    obj_func_val[tau+1,1] = train_obj_func(train_x, W[tau+1,],train_y, lambda)\n",
    "  } \n",
    "  # resulting values for the training objective function as well as the weights\n",
    "  return(list('vals'=obj_func_val,'W'=W))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent updating the weight vector\n",
    "\n",
    "bgd_update_weight <- function(W_prev, x, y_true, y_pred, lambda, eta) {\n",
    "  # MODIFY THIS FUNCTION For L2 REG\n",
    "  grad = -colMeans(matrix((y_true-y_pred),nrow=dim(x)[1],ncol=dim(x)[2]) * x)\n",
    "  return (W_prev - eta * (grad+lambda*W_prev))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Now letâ€™s compare SGD and BGD implementations of ridge regression from Step II:\n",
    "1. Load Task2A_train.csv and Task2A_test.csv sets.\n",
    "2. Set the termination criterion as maximum of 18 weight updates for BGD, which is equivalent to 18 x N weight updates for SGD (where N is the number of training data).\n",
    "3. Run your implementations of SGD and BGD while all parameter settings (initial values, learning rate etc) are exactly the same for both algorithms. During run, record training error rate every time the weights get updated. Create a plot of error rates (use different colors for SGD and BGD), where the x-axis is the number of visited data points and y-axis is the error rate. Save your plot in your Jupyter Notebook file for Question 6. Note that for every N errors for SGD in the plot, you will only have one error for BGD; the total length of the x-axis will be 20 x N.\n",
    "4. Explain (in your Jupyter Notebook file) your observation based on the errors plot you generated in Part c. Particularly, discuss the convergence speed and the fluctuations you see in the error trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to assignment code of question 5\n",
    "# import library\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "library(corrplot)\n",
    "\n",
    "# define read_data function\n",
    "read_data <- function(fname, sc) {\n",
    "  data <- read.csv(file=fname,head=TRUE,sep=\",\")\n",
    "  nr = dim(data)[1]\n",
    "  nc = dim(data)[2]\n",
    "  x = data[1:nr,1:(nc-1)]\n",
    "  y = data[1:nr,nc]\n",
    "  if (isTRUE(sc)) {\n",
    "    x = scale(x)\n",
    "    y = scale(y)\n",
    "  }\n",
    "  return (list(\"x\" = x, \"y\" = y))\n",
    "}\n",
    "\n",
    "## reading the data\n",
    "dtrain = read_data(\"./Task1C_train.csv\", TRUE)\n",
    "dtest = read_data(\"./Task1C_test.csv\", TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAANlBMVEUAAAAAAP9NTU1oaGh8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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set parameters for SGD \n",
    "max_epoch = 18\n",
    "epsilon = .001\n",
    "eta = .01\n",
    "lambda= 1.5\n",
    "\n",
    "# set parameters for BGD \n",
    "max_epoch = 18\n",
    "epsilon = .001\n",
    "eta = .01\n",
    "lambda= 1.5\n",
    "\n",
    "# Run SGD \n",
    "train_res1 = sgd_train(dtrain$x, dtrain$y, lambda, eta, epsilon, max_epoch)\n",
    "errors1 = get_errors(dtrain, dtest, train_res1$W) \n",
    "options(warn=-1)\n",
    "\n",
    "# Run BGD\n",
    "train_res2 = bgd_train(dtrain$x, dtrain$y, lambda, eta, epsilon, max_epoch)\n",
    "errors2 = get_errors(dtrain, dtest, train_res2$W)\n",
    "options(warn=-1)\n",
    "# Create a plot of error rates (SGD is blue and BGD is red), where the x-axis is the number of visited data \n",
    "# points and y-axis is the error rate.\n",
    "plot(errors1[,1], type=\"l\", col=\"blue\", xlab=\"iteration\", ylab=\"error\")\n",
    "lines(x=seq(1,19)*dim(dtrain$x)[1],y=errors2[,1],type=\"l\", lty=2, lwd=2, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explaination:\n",
    "From the plot aboce we can learn:\n",
    "1. The convergence speed of SGD is much faster than BGD's. The reason is that SGD only select on data point for updating the weights but BGD need to use the all dataset. Thus the computationally of BGD is much high than SGD's and this will happen epcaially in the large scale dataset.\n",
    "2. The decreasing trends of error rate are different. In SGD algorithm, the error will converge very fast after first few iterations but after that it will fluctuate around some values. It is easy to stunk in local minimun because it only randomly pick one data point for parameters update in every iteration. In practice, the region which SGD flucate around is close to the global minimum. By contrast, BGD can always reach the globalminimun after run enough times of iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
